{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "medium-friend",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import textwrap\n",
    "wrapper = textwrap.TextWrapper(width=70)\n",
    "\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.fastmath import numpy as jnp\n",
    "\n",
    "# to print the entire np array\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "rocky-vermont",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the data - download the dataset if no data_dir is specified.\n",
    "# so we have the data already in 'data/' for you\n",
    "\n",
    "# Importing CNN/DailyMail articles dataset\n",
    "train_stream_fnction = trax.data.TFDS('cnn_dailymail',\n",
    "                                 data_dir='../news_data/',\n",
    "                                 keys=('article', 'highlights'),\n",
    "                                 train=True)\n",
    "\n",
    "# This should be much faster as the data is downloaded already.\n",
    "eval_stream_fnction = trax.data.TFDS('cnn_dailymail',\n",
    "                                data_dir='../news_data/',\n",
    "                                keys=('article', 'highlights'),\n",
    "                                train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "important-ribbon",
   "metadata": {},
   "source": [
    "#### Create tokenize and detokenize functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "million-protein",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now need create helper functions to tokenize and detokenize data. Tokenise converts a text sentence to its\n",
    "# corresponding token list (i.e. list of indices). Also converts words to subwords.\n",
    "# similarly we need to have detokenize function to reconvert the tokens to its sentence\n",
    "\n",
    "def tokenize(input_str,EOS=1):\n",
    "    \"\"\" convert input string to a feature dictionary\"\"\"\n",
    "#     trax.data.tokenize method takes streams and returns streams, we user iter to have one elment stream\n",
    "    input_sting=next(trax.data.tokenize(iter([input_str]),\n",
    "                                       vocab_dir='vocab_dir/',\n",
    "                                       vocab_file='summarize32k.subword.subwords'))\n",
    "#     put EOS at the end of sentence\n",
    "    return list(input_string)+[EOS]\n",
    "\n",
    "def detokenize(input_integers):\n",
    "    \"\"\"convert input intergers to string\"\"\"\n",
    "    string_converted=trax.data.detokenize(input_integers,\n",
    "                                        vocab_dir='vocab_dir/',\n",
    "                                        vocab_file='summarize32k.subword.subwords')\n",
    "    \n",
    "    return wrapper.fill(string_converted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fantastic-czech",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language model and preprocessing\n",
    "# language models only predicts next work,we concatenate inputs with target and seperate them\n",
    "# with a seperator and concatenate them. Further padding masks are used 0s and 1s in input and targets \n",
    "# respectively. So the focus is model to pay attention on summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "pending-literature",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask tokens\n",
    "# SEP=0 #Padding or separator\n",
    "# EOS=1 #end of token sentence\n",
    "\n",
    "# # Now lets concatenate input tokens and targets using 0 as seperator\n",
    "def preprocess(stream):\n",
    "    \"\"\"get the data stream and seperate with 0, stream data comming with articles and summary\"\"\"\n",
    "    for (article,summary) in stream:\n",
    "        combine=np.array(list(article)+[EOS,SEP]+list(summary)+[EOS])\n",
    "        mask=[0]*(len(list(article))+2)+[1]*(len(list(summary))+1)\n",
    "        yield combine,combine,np.array(mask)\n",
    "\n",
    "# # make data pipeline as follows\n",
    "input_pipeline=trax.data.Serial(\n",
    "#     first tokennize\n",
    "    trax.data.Tokenize(vocab_dir='vocab_dir/',\n",
    "                        vocab_file='summarize32k.subword.subwords'),\n",
    "#     now use the above function preprocess\n",
    "    preprocess,\n",
    "#     need to filter out the strings longer than 2018\n",
    "    trax.data.FilterByLength(2048)\n",
    ")\n",
    "\n",
    "# # Apply above pipeline to both train and evaluation data\n",
    "train_stream=input_pipeline(train_stream_fnction())\n",
    "eval_stream=input_pipeline(eval_stream_fnction())\n",
    "\n",
    "# get one by one\n",
    "train_input,train_target,train_mask=next(train_stream)\n",
    "# train and target shoud be same language model\n",
    "assert sum((train_input-train_target)**2)==0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "amber-inclusion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single example mask:\n",
      "\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# prints mask, 0s on article, 1s on summary\n",
    "print(f'Single example mask:\\n\\n {train_mask}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "conservative-course",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single example:\n",
      "\n",
      " By . Ian Parkes, Press Association . Max Chilton has every confidence\n",
      "he will be retained by Marussia for a third consecutive season.\n",
      "Chilton started the campaign relatively strongly, claiming the best\n",
      "results of his Formula One career by finishing 13th in the season-\n",
      "opening race in Australia and again in Bahrain. In Monaco, however,\n",
      "team-mate Jules Bianchi stole Chilton's thunder as the Frenchman\n",
      "scored Marussia's first points from their four and a half years in F1\n",
      "with ninth place in Monaco. Centre of attention: Max Chilton remains\n",
      "hopeful of being retained by Marussia for the 2015 season . Since then\n",
      "Chilton has struggled for form and results, but the 23-year-old from\n",
      "Reigate in Surrey sees no reason why Marussia would not retain him for\n",
      "2015. 'I naturally want to stay with the team,' said Chilton. 'Like a\n",
      "lot of these things they filter down from the top, and there are a lot\n",
      "of rumours with regard to the top of the grid, with people moving\n",
      "around and you don't really know where you stand until then. 'I won't\n",
      "focus on that until later on in the year, but I'm confident I'll be\n",
      "here next year. I've had good races this year. 'I started off fairly\n",
      "strong, and okay the last few have not been particularly great, but I\n",
      "feel we've got to the bottom of that. Overall I've been consistent and\n",
      "had good results.' Chilton may yet be thanking Bianchi for that result\n",
      "in Monaco as the young Briton would like to believe it could play a\n",
      "key role in his own future. On track: The British driver joined the\n",
      "team in 2013 and finished every race of his debut season . Those two\n",
      "points mean Marussia lie ninth in the constructors' title race ahead\n",
      "of both Sauber and Caterham. If Marussia can hold on to that position\n",
      "the financial rewards would be considerable, which in turn may mean\n",
      "Chilton not having to find the cash to fund his seat. 'Marussia have a\n",
      "good future, especially if we hold off Sauber for ninth. That would\n",
      "really build up momentum,' added Chilton. 'That would be a big help to\n",
      "the team financially if we could do that as it would help us develop\n",
      "the car for next year. 'If the team gets this ninth then we might not\n",
      "need to worry about that (his financial situation). That's just me\n",
      "really thinking of the bigger picture. 'I've not properly looked into\n",
      "it, but I'd like to think I could continue to help the team develop\n",
      "over the next couple of years.'<EOS><pad>Chiltonis hopeful of racing\n",
      "at Marussia for a third straight season . Briton's form has dipped in\n",
      "recent races after finishing 13th in Bahrain . But he is confident of\n",
      "staying at Marussia beyond this season .<EOS>\n"
     ]
    }
   ],
   "source": [
    "# prints: [Example][<EOS>][<pad>][Example Summary][<EOS>]\n",
    "print(f'Single example:\\n\\n {detokenize(train_input)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "muslim-desktop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs are in different lengths and padding them with 0s will wast computational resource so good approach \n",
    "# would be group the strings to specific sizes and process, ww use buckets to create batched generators\n",
    "# buckets are defined based on boundaries and batch sizes, batch size[i] signifies the \n",
    "# batch size for the items with length < boundaries[i], so we use batch size 4 of length<512, 8 of length<256,16 of sentence lewnght <128 so on\n",
    "\n",
    "boundaries =[128,256,512,1024]\n",
    "batch_size=[16,8,4,2,1]\n",
    "\n",
    "# now create the stream\n",
    "train_batch_stream=trax.data.BucketByLength(boundaries,batch_size)(train_stream)\n",
    "eval_batch_stream=trax.data.BucketByLength(boundaries,batch_size)(eval_stream)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "funky-craps",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1024)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# different articles will be produced every time\n",
    "input_batch,_,mask_batch=next(train_batch_stream)\n",
    "input_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "dangerous-language",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   27,  6327, 19480,   649,  3331,   540,   163, 10871,  5258,\n",
       "         7511,    22,  4194,   320,   399,    28, 18870,   330, 15702,\n",
       "          132,   163, 25209,  1560,  8833,   186,  5564,   213,   157,\n",
       "         3408,   134,  1353,    36,   527,    15,  8866, 14161,     5,\n",
       "            3, 10851,   800, 19480,   649,  3331,  2958,  6211,  5861,\n",
       "          142,  1353,  3156,  8751,  2631,    78,   213,  6327,     6,\n",
       "          766,  9217,   138,    78,  2613,  4979,  7511,    22,   980,\n",
       "          213,  5556,   186,  1410,   320,  6909,    95,   186,   399,\n",
       "            3,  6211,  5861,   142,     2,  1874,     2,   721,   320,\n",
       "          399,    36,   157,  1124,   320,  6166,   213,  8833,   179,\n",
       "           78,    50, 16612,   186,   102,   290,   181,   409,   469,\n",
       "         1241,   132,    41,    25,   217,   320,   211,   213,  8833,\n",
       "        14643,  7023,     3,  6327, 19480,   649,  3331,  2958,  6211,\n",
       "         5861,   142,     2,   231,     2,   540,   163, 10871,  5258,\n",
       "         7511,    22,  4194,   320,   399,    28, 18870,   330, 15702,\n",
       "          132,   163, 25209,  1560,  8833,   186,  5564,   213,   157,\n",
       "         3408,   134,  1353,    15,  8866, 14161,     5,     2,  2475,\n",
       "         3106,   379,   312,  6211,  5861,   142,  1411,   320, 19129,\n",
       "           14,   213,  4904,  1248,   213,   157,   382,   320,   134,\n",
       "           70,   213,    36,  1779,  1353,   213,    60,    78,   213,\n",
       "         1610,    70,    22,  2091,   134,   412, 19480,   649,  3278,\n",
       "         9100, 13743,  2475,  3106,  1782,    13,  1411,   320,   208,\n",
       "          409,   213,  3454,  1019,    28,   227,  1082,   757,  1269,\n",
       "          824,  8833,    61,     2,   186,    13,   742,   320,  2883,\n",
       "         2035, 27634,     4,  2088,     2,   285,  2314,   107,  2475,\n",
       "         3106,  4617, 27634,    80,  6211,  5861,   142,   793, 27439,\n",
       "         9275,  1628,     9,  6327,  3863,     3,  3106,   625,    28,\n",
       "          393,     6,   282, 11175,    78,  2102,  1019,    28,   270,\n",
       "         2214,  1935,  1166,  1019,   163, 20999,  4558,  3214,   285,\n",
       "           22,   272,  2441,  1353, 10493, 10027,    84,  1782,   312,\n",
       "           13,   127,  2035, 27634,     4,  2475,  7339, 27634,   391,\n",
       "          186,    22,   127,  2035, 27634,     4, 11199,  4617, 27634,\n",
       "          391,   179,     2,    22,   206,   103,   412,   122,    13,\n",
       "         1353,  1821,   103,   320,   130,   269,  1650,  2998,  3898,\n",
       "          127,  6211,  5861,   142,     3,  6211,  5861,   142,  3891,\n",
       "         2685,   213,  1832,   920,  1248,    36,   527,    15,  8866,\n",
       "        14161,     5,    78,    15,  3004,   843, 11969,     7,    69,\n",
       "         1353,    28,   608,  2951,  3454,     3,    69,  7157,   320,\n",
       "         7502,     3,    69,  7157,   320,   213,  3786, 24747,     3,\n",
       "          200,    22,   170,  4351,   213,   321,     3,   520,    38,\n",
       "          213,    55,   141,    90,    51,    49,  1399,  2002, 12812,\n",
       "            5, 16292,   150,  1196,   320,  5279,  3521,  1025,  3334,\n",
       "         6731,     4,   636,   186,    28,  2753,     6,   104,     6,\n",
       "          292,   157,  1353, 16292,   320,  3521,  1025,  3334,  6731,\n",
       "            4,  1248,  1839,  6341,   285,    25,    19,   689,   177,\n",
       "        11648,     3,  6211,  5861,   142,     2,    28,   280,  8986,\n",
       "         1779,  1056,   132, 15022,    47,     2,    43,   793,  3106,\n",
       "           22,   742,    22,   540,   117, 21232,    17,    80,   691,\n",
       "         3000,   951,  7759,     2,   186,    22,   127,  3106,   793,\n",
       "          134,    22, 12600,    15, 24095,  1782,    69, 29725,     4,\n",
       "            5,  2170,   196,   130,  3076, 19480,   216,     3,    69,\n",
       "         1063,   320,  1151,    28,   608,  2951, 22681,  3454,  1779,\n",
       "          141,  4708,   320,  1205,    28,  1400,   456,   525,  1782,\n",
       "           13,  1353,  4418,   320,   191,    44,  9233,    78,   130,\n",
       "          138,   278,     2,    35,    13,   742,     2,    13,    49,\n",
       "        29725,     4,    26,     3,    13, 29725,     4,   183,   540,\n",
       "          320,   273,   278,   186,  1399,   130,   938,  2685,   824,\n",
       "          166,   130,  1280,  2067,  1793, 29725,     4,    26,   477,\n",
       "         2002,    27, 14380,  1019,   213, 19480,   649,  3088,   285,\n",
       "         3106,  1353,  1490,   412,    28,  5554,   132,    28,   864,\n",
       "          648,   186,  1654,    28,  1602,  2685,  2754,  2195,     3,\n",
       "         3106,   625,    28,   393,     6,   282, 11175,    78,  2102,\n",
       "         1019,    28,   270,  2214,  1935,  1166,  1019,   163, 20999,\n",
       "         4558,  3214,    22,   272,  2441,  1353, 10493, 10027,    84,\n",
       "         2104,     1,     0, 10851,   800, 19480,   649,  3331,  2958,\n",
       "         6211,  5861,   142,  1353,  3156,  8751,  2631,    78,   213,\n",
       "         6327,     6,   766,  9217,   138,    78,  2613,  4979,  7511,\n",
       "           22,   980,   213,  5556, 16346, 27439,  6774,  1628,    69,\n",
       "         1504,   297,   157,   320,  7424,    28, 18870,   330,   132,\n",
       "          163, 25209,  1560,  8833,   186, 10038,  5564,   285,   213,\n",
       "          157,  1353,  2475,  3106,   527,   213, 19480,   649, 16346,\n",
       "        27439,  6774,  1628,  3106,   625,    28,   393,     6,   282,\n",
       "        11175,    78,  2102,  1019,    28,   270,  2214,  1935,  1166,\n",
       "         1019,   163, 20999,  4558,  3214,    22,   272,  2441,  1353,\n",
       "        10493, 10027,    84, 16346, 27439,  6774,  1628,  3209,  8986,\n",
       "         6211,  5861,   142,   793,  3106,    22,   742,    22,   540,\n",
       "          117, 21232,    17,    80,   691, 13677,  2104,     1,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0],\n",
       "       [    9, 15501,    81,  1353,  2704,   132,   213,  4233,   527,\n",
       "          915,  9284, 17501,  1779,   272,  6122,   150,  9982,   527,\n",
       "         3098, 12928,   360,   379,  6711,  5626,    27,  8305,   692,\n",
       "          233,    28,  3957, 23319,    17,   320,   583,   132,    28,\n",
       "         4233,  3094,  1480,    40,    46,  9960,  4002, 10409,  1248,\n",
       "           28, 11349,  1453,  2550,     3,     9, 15501,    81,  1353,\n",
       "         2704,   132,   213,  4233,   527,   915,  9284, 17501,     2,\n",
       "           28,   779,     6,   527,     6,   290,     2,  1779,   272,\n",
       "         6122,   150,  9982,   527,  3098, 12928,   360,     3,     9,\n",
       "         1570,     6,   104,     6,   292,     2,   527, 11253,   494,\n",
       "            2,   515, 23912,     5,     2,   793,  8305,   692,   213,\n",
       "        15501,    81,   316, 11984,    20,    40,  2274,  3706,   390,\n",
       "          171,   213,  5327,    78,   481,   381,     3,  9284, 17501,\n",
       "            2,   163,  4599, 21980,    17, 11440,   328,  6910,     2,\n",
       "          793,   105,    22,    40,    92,  1004,  7270,   131,  1790,\n",
       "           61, 23319,    16,   320,   583,   132,   213, 19086,  6616,\n",
       "         1639,  3094,     3,   187,    22,  1353, 11529,   809,  7618,\n",
       "        19862, 18549,     5,   812,    78,  2102,     2,    22,  1353,\n",
       "          793,   213,   347,  1353,    36,   527,   213,  4947, 12358,\n",
       "        18549,     5,    40,   553,     3,   342,     2,  9284, 17501,\n",
       "        13488,  2173,   166,    22,   229,   213,  8534,  5137,    58,\n",
       "          527,    15,   290,   310,   102,    15,  1819,     6,  2583,\n",
       "         7609,    64,    78,   213,   228,     2,   213,   924,  1613,\n",
       "            3, 17787,    16,   134,  1838,  2872,  2196,  1019,   177,\n",
       "            2,   213,  4547,   527,   213, 13170,   793,   213,  4022,\n",
       "         3611,    56,   229,    28,   139,  1839, 22993,     2,    36,\n",
       "          527,   213,  4947,   285,    23,   413,   171,   824,   924,\n",
       "         1782,    52,   229,    86,   166,   527,   109,   263,  8272,\n",
       "         2950,    28,   186,  2439, 16003,    33,  1435,    19,   416,\n",
       "          320,  2173,  2002,   207,  1003,   134,   163,   406,     6,\n",
       "          719,  9457,  6369,   186,  1471,   134,   155, 12424,  1019,\n",
       "           72,    91,     3,    69,  1353,    43,  3523,   320,  2560,\n",
       "           64,   571,   926, 12572,   854,   126,   186,  1153,  1687,\n",
       "         2094,  1368,     3, 18103,  1826, 24813,    78,     2, 17347,\n",
       "           16,  1019,   213,  6711,  5626,    27,     2,   127,   163,\n",
       "         8305,   181,  3198,  9284, 17501,     7,     5,  1319,    78,\n",
       "          481,   381,   102,  2619,   163, 10395,  2785,  1173,  2495,\n",
       "          151,  1530, 14993,  1248,    36,   527,    15, 25460,     3,\n",
       "          207,    25,   793,   213,  3957,    40,    46,  3359,  1838,\n",
       "         9284, 17501,     3,   312,   213,  8305,   181,  1353,   953,\n",
       "           71,    15,   278,   691,    36,   527,    15,   310,     2,\n",
       "          163, 10395,  2785,  1173, 16090,  7757,  3800,   479,  3957,\n",
       "         1353,   233,   320,  1151,   239,   605,    50,  1200,   661,\n",
       "         1732,     3,   380,   527,   213,  5324,   233,   809,   213,\n",
       "        11253,   494,   278,   527,   915,  9284, 17501,     3,     9,\n",
       "         1570,     6,   104,     6,   292, 13488,  2173,   102, 18250,\n",
       "         5995,   150,  9982,   527,  3098, 12928,   360,   379,   200,\n",
       "          213,    94,  7379,  7829,  5327,  1353,   133,  7511,   213,\n",
       "        11349,  1453,  2550,  1353,   995,   463,  1838,   213,  3094,\n",
       "         2278,   186,   213,  1955, 15501,    81,  1353,  2704,     3,\n",
       "         1678, 24813,    78,   127,  3611,     9,  8305,   181,   980,\n",
       "          213,  2495,   151,  1530, 14993,   132,   213,  4233,   186,\n",
       "          213,  1955, 15501,    81,   629,  3957,   132,   213,  3094,\n",
       "         1782,   198,  1353,    28, 11349,  1453,  2550,   132,   979,\n",
       "          527,   213,  2278,  1782,     9,  9429,  2362,   278,   186,\n",
       "         1353,   875,   213,  1955,  3957,     3,    69,   127,    22,\n",
       "          742,   103,    40,  2274,  3706,   186,   285,    22,    40,\n",
       "          553,   103,    28,   335,   390,   171,  1782,     9,  3957,\n",
       "           40, 17544,    17,  4553,   186,    77,  1353,    92,   837,\n",
       "          181,   304,   132,   213,  3094,     3,   514,  8305,   181,\n",
       "         3198,  9284, 17501,     7,     5,  1319,   102,  2619,   163,\n",
       "        10395,  2785,  1173,  2495,   151,  1530, 14993,  1248,    36,\n",
       "          527,    15, 25460, 11969,     7,     9,  3957,    40,    46,\n",
       "         1955,  1019,    87,    55,   186,    77,    25, 10530,  9897,\n",
       "            5,   132,   213, 14825,  1782,    27,   756, 10529,  3490,\n",
       "          233,   163, 10395,  2785,  1173,  3957,  1248,  7025,   116,\n",
       "          661,  1732,   186,   213,  1275,   527,   583,  1353,  7522,\n",
       "          320,  1151, 23319,   232,     3,   198,    25,    92,  3796,\n",
       "        13557,  2002,    34,    28,  1602,     2,   213, 22780,  1779,\n",
       "         1526,    64,   213,   756, 10529,  3490,   127,  3611,     9,\n",
       "          662,   527,   824,  3957,  1353, 15632,  5582,     2,  3341,\n",
       "           28,  3957,   320,  2276,   527, 23319,   232,   229, 21822,\n",
       "         6616,   478,     3,    27, 15501,    81,   629,  3957,   233,\n",
       "         1955,   132,  9284, 17501,     7,     5,  3094,  1480,  1353,\n",
       "         9960,  4002, 10409,  1248,    28, 11349,  1453,  2550, 11969,\n",
       "            7,  5553,    62,    18,    46,  3732,   186, 17849,  1782,\n",
       "          198,   229,    28,   976,  1430,   527,   662,   186, 26848,\n",
       "            3,    56,  1353,   583,   132,   213,  4947,   138, 21845,\n",
       "          478,  2002, 14457,   100, 26648,     2, 14038,     2,   127,\n",
       "         3611,    69,  3253,   156,    22,  2408,   123,   213,  4233,\n",
       "          186,   132,   213,  3291,    35,  3161,     7,    26, 15676,\n",
       "          213,  3957,  1782,    69,   206,    19, 22269,   103,  1353,\n",
       "          132,   213,  3094,  2002,   368, 26648,   533,    78,   320,\n",
       "          476,  9284, 17501,     2,  1779,    40,    46,   133, 22174,\n",
       "           26,  1838,    15, 11440,   328,  1082,     2,    40,  4133,\n",
       "           28,   422,   527,  6034,   102,   144,   346,  1847,   320,\n",
       "          662,  1019,    15,   290,   310,  1782,    69,  1353,    19,\n",
       "        19680,   113,  5540,  3898,    22,   127,  1782,    52,     7,\n",
       "            5,    92, 24301,  1019,  2754,     7,     5,  2195,   186,\n",
       "           22,  1094,  2269,  1019,   285, 11585,     1,     0,  6930,\n",
       "         8217,    58,  2704,   132,   213,  4233,   527,   915,  9284,\n",
       "        17501,     2,  1570,     2,   527,   515, 23912,     5, 16346,\n",
       "        27439,  6774,  1628,     9,   779,     6,   527,     6,   290,\n",
       "          127, 11984,    20,    40,  2274,  3706,   390,   171,   131,\n",
       "         1353,   233, 16346, 27439,  6774,  1628,  9284, 17501,   127,\n",
       "           22,    40,    92,  1004,  7270,   131,  1790,    61, 23319,\n",
       "           16,   320,   583,   132,  3094, 16346, 27439,  6774,  1628,\n",
       "            9,  3957,    40, 17544,    17,  4553,   186,    77,  1353,\n",
       "           92,   837,   181,   304,   132,   213,  3094, 16346, 27439,\n",
       "         6774,  1628,  9284, 17501, 13488,  2173,   166,    22,   229,\n",
       "          213,  8534,  5137,    58,   527,    15,   290,   310,  2104,\n",
       "            1,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see the corresponding iteger values\n",
    "input_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "double-iraqi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article and summary:\n",
      "\n",
      " A Baltimore Orioles fan got an unexpected surprise when he stopped to\n",
      "help a motorist trapped in an overturned truck and realized the man\n",
      "helping him was one of his sporting idols. Diehard Orioles fan Mike\n",
      "Soukup was driving southbound on the Baltimore-Washington Parkway on\n",
      "Monday afternoon when he saw the accident and decided to pull over and\n",
      "help. Soukup, 55, started to help one man try to push the truck back\n",
      "on its wheels and after four or five others joined in they were able\n",
      "to get the truck upright. Baltimore Orioles fan Mike Soukup, right,\n",
      "got an unexpected surprise when he stopped to help a motorist trapped\n",
      "in an overturned truck and realized the man helping him was his\n",
      "sporting idols, Chris Davis . When Soukup turned to congratulate the\n",
      "achievement with the man next to him - the one who was the first on\n",
      "the scene - he recognized him as Orioles corner infielder Chris Davis.\n",
      "'I turned to high five the guy for a good job done getting this truck\n",
      "up, and I thought to myself, \"Man, that looks like Chris Davis,\"'\n",
      "Soukup told The Baltimore Sun. Davis received a 25-game suspension on\n",
      "Friday for a second failed drug test for an amphetamine that he later\n",
      "revealed was Adderall. 'When I said, \"Chris?\" and he said, \"Yeah,\"\n",
      "back, he did it as if I was saying it to my best friend Bob,' said\n",
      "Soukup. Soukup posted about the chance meeting with one of his\n",
      "sporting idols on his Facebook page . 'He was a real nice guy. He\n",
      "talked to everybody. He talked to the EMTs. But he should wear the No.\n",
      "19 all the time just so we can tell.' Medics transported three\n",
      "individuals to Maryland Shock Trauma Center and a 61-year-old man was\n",
      "transported to Shock Trauma with serious injuries that were not\n",
      "considered life threatening. Soukup, a local musician who lives in\n",
      "Severn, also told Davis he thought he got 'screwed' by Major League\n",
      "Baseball, and he said Davis told him he appreciated his sentiment.\n",
      "'He’s pretty much my favorite Oriole. He seems to be a real nice\n",
      "humble guy who just happens to hit a ball really far. 'I was supposed\n",
      "to make more stops on my way home, but I thought, I can’t. I’ve got to\n",
      "go home and tell my wife about this because my cell phone wasn’t\n",
      "working.' A spokesman for the Orioles confirmed that Davis was listed\n",
      "as a witness in a police report and signed a statement about what\n",
      "happened. Davis received a 25-game suspension on Friday for a second\n",
      "failed drug test for an amphetamine he later revealed was Adderall\n",
      ".<EOS><pad>DiehardOrioles fan Mike Soukup was driving southbound on\n",
      "the Baltimore-Washington Parkway on Monday afternoon when he saw the\n",
      "accident . He helped another man to rescue a motorist in an overturned\n",
      "truck and afterwards realized that the man was Chris Davis of the\n",
      "Orioles . Davis received a 25-game suspension on Friday for a second\n",
      "failed drug test for an amphetamine he later revealed was Adderall .\n",
      "Local musician Soukup told Davis he thought he got 'screwed' by MLB .<\n",
      "EOS><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><\n",
      "pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><\n",
      "pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><\n",
      "pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><\n",
      "pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><\n",
      "pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><\n",
      "pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><\n",
      "pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><\n",
      "pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><\n",
      "pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><\n",
      "pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><\n",
      "pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><\n",
      "pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><\n",
      "pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><\n",
      "pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><\n",
      "pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><\n",
      "pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><\n",
      "pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><\n",
      "pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><\n",
      "pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><\n",
      "pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><\n",
      "pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><\n",
      "pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><\n",
      "pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "# above input batch consists:\n",
    "#     all the values corresponding to words, first 1 represents the <EOS> of the article followed by 0 (pads)\n",
    "#     after the first 0, other values show the summary words and the second 1 represent the <EOS> tag for summary\n",
    "#       All the other 0s are to maintain the consistancy of the  length - max length specified in the bucket \n",
    "# show the processes input data batch\n",
    "print(\"Article and summary:\\n\\n\",detokenize(input_batch[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "solid-consultation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so the structure is article <EOS><PADs>article summary <EOS><pads>\n",
    "# loss is taken only on the summary using cross entropy as loss function\n",
    "#  Now create helper functions to create tensor and to display tensors using jax numpy array\n",
    "\n",
    "def create_tensor(tensor):\n",
    "    \"\"\"input list of lists out put a tensor\"\"\"\n",
    "    return jnp.array(tensor)\n",
    "\n",
    "def display_tensor(tensor,name):\n",
    "    \"\"\" display the name and tensor\"\"\"\n",
    "    print(f'{name} shape: {tensor.shape}\\n')\n",
    "    print(f'{tensor}\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-digest",
   "metadata": {},
   "source": [
    "# try with dummy data and build attention - dot product\n",
    "$$\n",
    "\\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}+{M}\\right) V\\tag{1}\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "proved-complex",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dotproduct attention\n",
    "def DotProductAttention(query,key,value,mask):\n",
    "    \"\"\" dot product self attention\n",
    "    args: query - jax.intepreters.xla.DeviceArray: array of query representations with shape L_q by d\n",
    "    key jax.intepreters.xla.DeviceArray: array of query representation with shape L_k by d\n",
    "    value jax.intepreters.xla.DeviceArray: array of value representation with shpeL_k by d where L_v=L_k\n",
    "    mask jax.interpreters.sla.DeviceArray: attention mask, gates attention with shape L_q by L_k ( this is due to dot product)\n",
    "    returns jax.intperters.xla.DeviceArray: self-attention array for q,k,v arrays L_q by L_k\"\"\"\n",
    "    \n",
    "    assert query.shape[-1]==key.shape[-1]==value.shape[-1],\"Embeedding dimesions of q,k,v must be same\"\n",
    "#     get the depth dimentionality of the query embedding  for the scaling down the dot product\n",
    "    depth=query.shape[-1]\n",
    "    \n",
    "#     get the scaled query key dot product according to formula above\n",
    "    dots=jnp.amtmul(query,jnp.swapaxes(key,-1,-2))/jnp.sqrt(depth)\n",
    "    \n",
    "#     now apply the mask\n",
    "    if mask is None:\n",
    "        dots=jnp.where(mask,dots,jnp.full_like(dots,-1e9))\n",
    "    \n",
    "#     softmax\n",
    "    logsumexp=trax.fastmath.logsumexp(dots,axis=-1, keepdims=True)\n",
    "    \n",
    "#     now get the exponnential of dots minus logsumexp to get softmax\n",
    "    dots=jnp.exp(dots-logsumexp)\n",
    "#      now multiply by values to get the attention\n",
    "    attention=jnp.matmul(dots,vlaue)\n",
    "    \n",
    "    return attention\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-coaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now inpmelent the causal attention: multi headed attention with mask to attend only the words that occured before\n",
    "# 1. copute attention heads - get input with dimention (batch size, seqlen,n_heads X d_head) then splits the last(depth)\n",
    "# dimension and stacks it to the zeroth dimension to allow matrix multiplication (batch_size X n_heads,seqlen,d_head)\n",
    "\n",
    "# 2. dot product self attention\n",
    "# 3. compute attention output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
