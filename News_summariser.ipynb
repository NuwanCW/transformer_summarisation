{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "medium-friend",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import textwrap\n",
    "wrapper = textwrap.TextWrapper(width=70)\n",
    "\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.fastmath import numpy as jnp\n",
    "\n",
    "# to print the entire np array\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "rocky-vermont",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the data - download the dataset if no data_dir is specified.\n",
    "# so we have the data already in 'data/' for you\n",
    "\n",
    "# Importing CNN/DailyMail articles dataset\n",
    "train_stream_fnction = trax.data.TFDS('cnn_dailymail',\n",
    "                                 data_dir='../news_data/',\n",
    "                                 keys=('article', 'highlights'),\n",
    "                                 train=True)\n",
    "\n",
    "# This should be much faster as the data is downloaded already.\n",
    "eval_stream_fnction = trax.data.TFDS('cnn_dailymail',\n",
    "                                data_dir='../news_data/',\n",
    "                                keys=('article', 'highlights'),\n",
    "                                train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "important-ribbon",
   "metadata": {},
   "source": [
    "#### Create tokenize and detokenize functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "million-protein",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now need create helper functions to tokenize and detokenize data. Tokenise converts a text sentence to its\n",
    "# corresponding token list (i.e. list of indices). Also converts words to subwords.\n",
    "# similarly we need to have detokenize function to reconvert the tokens to its sentence\n",
    "\n",
    "def tokenize(input_str,EOS=1):\n",
    "    \"\"\" convert input string to a feature dictionary\"\"\"\n",
    "#     trax.data.tokenize method takes streams and returns streams, we user iter to have one elment stream\n",
    "    input_sting=next(trax.data.tokenize(iter([input_str]),\n",
    "                                       vocab_dir='vocab_dir/',\n",
    "                                       vocab_file='summarize32k.subword.subwords'))\n",
    "#     put EOS at the end of sentence\n",
    "    return list(input_string)+[EOS]\n",
    "\n",
    "def detokenize(input_integers):\n",
    "    \"\"\"convert input intergers to string\"\"\"\n",
    "    string_converted=trax.data.detokenize(input_integers,\n",
    "                                        vocab_dir='vocab_dir/',\n",
    "                                        vocab_file='summarize32k.subword.subwords')\n",
    "    \n",
    "    return wrapper.fill(string_converted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "fantastic-czech",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language model and preprocessing\n",
    "# language models only predicts next work,we concatenate inputs with target and seperate them\n",
    "# with a seperator and concatenate them. Further padding masks are used 0s and 1s in input and targets \n",
    "# respectively. So the focus is model to pay attention on summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "pending-literature",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask tokens\n",
    "# SEP=0 #Padding or separator\n",
    "# EOS=1 #end of token sentence\n",
    "\n",
    "# # Now lets concatenate input tokens and targets using 0 as seperator\n",
    "def preprocess(stream):\n",
    "    \"\"\"get the data stream and seperate with 0, stream data comming with articles and summary\"\"\"\n",
    "    for (article,summary) in stream:\n",
    "        combine=np.array(list(article)+[EOS,SEP]+list(summary)+[EOS])\n",
    "        mask=[0]*(len(list(article))+2)+[1]*(len(list(summary))+1)\n",
    "        yield combine,combine,np.array(mask)\n",
    "\n",
    "# # make data pipeline as follows\n",
    "input_pipeline=trax.data.Serial(\n",
    "#     first tokennize\n",
    "    trax.data.Tokenize(vocab_dir='vocab_dir/',\n",
    "                        vocab_file='summarize32k.subword.subwords'),\n",
    "#     now use the above function preprocess\n",
    "    preprocess,\n",
    "#     need to filter out the strings longer than 2018\n",
    "    trax.data.FilterByLength(2048)\n",
    ")\n",
    "\n",
    "# # Apply above pipeline to both train and evaluation data\n",
    "train_stream=input_pipeline(train_stream_fnction())\n",
    "eval_stream=input_pipeline(eval_stream_fnction())\n",
    "\n",
    "# get one by one\n",
    "train_input,train_target,train_mask=next(train_stream)\n",
    "# train and target shoud be same language model\n",
    "assert sum((train_input-train_target)**2)==0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "amber-inclusion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single example mask:\n",
      "\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# prints mask, 0s on article, 1s on summary\n",
    "print(f'Single example mask:\\n\\n {train_mask}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "conservative-course",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single example:\n",
      "\n",
      " One of the country’s biggest rail terminals will be ‘effectively\n",
      "closed’ today because of over-running engineering works. There will be\n",
      "no trains in or out of King’s Cross in London due to delays to Network\n",
      "Rail works north of the station. The disruption comes on one of the\n",
      "busiest travel days of the year, as thousands of people try to return\n",
      "home after visiting family for Christmas. Scroll down for video .\n",
      "Services in and out of London Kings Cross station have been cancelled\n",
      "today, it has been announced . Frustration: Travellers at the London\n",
      "station, one of the busiest in the country, where services are\n",
      "cancelled . The disruptions at the station, which is managed by\n",
      "Network Rail, will affect those planning to travel on East Coast,\n",
      "First Hull Trains, Grand Central and Great Northern services. East\n",
      "Coast Trains made the announcement on its website yesterday evening,\n",
      "where it advised passengers to delay their travel if possible. It also\n",
      "said that a revised timetable is currently being developed and will be\n",
      "made available as soon as possible. Customers have been advised they\n",
      "need to start or finish their journey at Finsbury Park in north\n",
      "London, with at least one change of train because of the overrun works\n",
      "in the Holloway area. A reduced service to and from the station will\n",
      "go ahead as planned on Sunday, with trains  leaving up to 20 minutes\n",
      "earlier than normal from King's Cross. Last night Passenger Focus, the\n",
      "independent watchdog, described the delays as ‘frustrating’. East\n",
      "Coast made the announcement on their website and apologised for\n",
      "disruption on what is 'an already very busy travel day immediately\n",
      "following the Christmas break' They follow years of complaints from\n",
      "passengers that Britain’s railways effectively ‘shut down’ over the\n",
      "Christmas and New Year holiday. The chaos is heightened by the fact\n",
      "that the West Coast Main Line is already closed for engineering works\n",
      "over the Christmas period. The route is shut between London Euston and\n",
      "Hemel Hempstead in Hertfordshire until Monday, and also closed between\n",
      "Stafford and Crewe until Sunday. Last night travellers vented their\n",
      "frustration with the King’s Cross closure on social media, saying\n",
      "Network Rail had ‘explaining to do’. The station is the southern\n",
      "terminus of the East Coast Main Line, which provides services to major\n",
      "cities including Leeds, Newcastle and Edinburgh. East Coast spokesman,\n",
      "Paul Emberley, said: 'Network Rail has apologised to passengers for\n",
      "the inevitable delays to their travel plans on Saturday as a result of\n",
      "the overrunning engineering works. 'East Coast is particularly sorry\n",
      "too for the inconvenience to its customers as a result, on what we\n",
      "know is an already very busy travel day immediately following the\n",
      "Christmas break. Sources said that the station would effectively be\n",
      "closed  due to delays to works north of the station . 'For customers\n",
      "intending to start or finish their journey at King's Cross,\n",
      "consideration should be given to deferring travel plans to either\n",
      "Sunday or Monday. 'We're working hard over the holiday period to make\n",
      "the necessary adjustments to our timetable as a consequence, and to\n",
      "provide as much information as we can.' National Rail said on its\n",
      "website that services to and from King's Cross would be 'significantly\n",
      "disrupted' tomorrow. It said there will be no trains before 10am\n",
      "tomorrow and over the weekend, many trains will start or terminate at\n",
      "Doncaster, Peterborough, Stevenage or Finsbury Park. Meanwhile, First\n",
      "Hull Trains will run two trains in each direction each day to and from\n",
      "London St Pancras International instead of London Kings Cross. Great\n",
      "Northern are diverting some trains to and from Moorgate between 8.30am\n",
      "and 7.30pm. It said:  'A revised timetable is currently being\n",
      "developed and will be made available as soon as possible. Please check\n",
      "our website regularly for updates. 'Passengers travelling to and from\n",
      "central London may use London Underground services between Finsbury\n",
      "Park and central London. Customers will need to start or finish their\n",
      "journey at Finsbury Park in north London, with at least one change of\n",
      "train, according to East Coast Trains . 'Buses will run between London\n",
      "Kings Cross and Finsbury Park when there is no train or London\n",
      "Underground service.' The rail company said that the work was part of\n",
      "a £200 million Christmas investment programme. It is one of 300\n",
      "projects being undertaken over the holidays across 2,000 sites up and\n",
      "down the country by some 11,000 railway engineers. A spokesman said:\n",
      "'What has happened is really regrettable and unfortunate, but it is a\n",
      "small part of a massive amount of engineering investment taking place\n",
      "over Christmas.' The spokesman added that 4.5 million passengers use\n",
      "the railways on average every day, compared with two million a day\n",
      "over the holidays. The company confirmed the disruption at around\n",
      "8.30pm on its Twitter page - hours after it was first announced by\n",
      "East Coast Trains, which said: 'No service between Kings Cross &\n",
      "Finsbury Park tom due to track work in the Holloway area which has\n",
      "taken longer then expected. Apologies.' Hundreds were forced to take\n",
      "to the roads yesterday after rail services were virtually non-existent\n",
      "today . A reduced service will also operate to and from the terminal\n",
      "on Sunday as planned, with journeys possibly being re-timed or taking\n",
      "longer than expected, according to Network Rail. David Sidebottom,\n",
      "passenger director at the independent watchdog Passenger Focus, said:\n",
      "'Investment in maintenance and improvement is necessary, and we\n",
      "passengers understand that. 'But overrunning works that disrupt\n",
      "already-limited festive travel are frustrating. 'Our research is\n",
      "clear: passengers want to be kept on the train wherever possible, they\n",
      "want to know before buying a ticket if part of the journey will be by\n",
      "bus, and they want plenty of staff on hand to signpost where to go and\n",
      "what to do. 'We will be looking to see that operators and Network Rail\n",
      "are doing all in their power to alert passengers, to help them make\n",
      "alternative arrangements and to make it easy for them to claim refunds\n",
      "or compensation.' Several frustrated passengers have taken to Twitter\n",
      "to vent their anger. Liam Gladdy said: 'Something must have gone very,\n",
      "very wrong for @networkrail at Kings Cross if they're looking at\n",
      "overrunning by 2 days for 2 days work.' While Amelie Soleil wrote:\n",
      "'Ahhh what's that? All trains are cancelled coming though Kings Cross\n",
      "tomorrow. Brilliant. Nice to see that £200 train ticket was worth it.'\n",
      "Emily Clifton said: 'Trains all cancelled from King's Cross tomorrow\n",
      "literally what am i supposed to do,' while another user said 'The UK\n",
      "are so unprepared. Today it's just like 'Oh, by the way, we're closing\n",
      "King's Cross for a few days.' It comes after a day of travel\n",
      "disruption on the railways as every single major artery in Britain was\n",
      "closed on Boxing Day. Trains between England and Scotland or Wales on\n",
      "the East Coast, West Coast or Great Western mainlines - and the\n",
      "Midland, Cross Country and East Anglia lines remained shut yesterday.\n",
      "The near-non-existent Boxing Day service has returned this year\n",
      "despite a fierce political battle over the shutdown dating back to at\n",
      "least 2007. Labour and Tories have repeatedly traded accusations over\n",
      "the lack of service, which affects football fans, families without\n",
      "cars and shop workers in the Boxing Day sales. Apart from airport\n",
      "shuttles and Eurostar, only Chiltern, Scotrail, Southeastern and\n",
      "Southern ran trains yesterday. A Department for Transport spokesman\n",
      "said: ‘Network Rail and train companies have ensured that a large part\n",
      "of the railway will remain open during the Christmas/New Year period\n",
      "and alternative routes are provided where lines are\n",
      "closed.’<EOS><pad>EastCoast, First Hull Trains, Grand Central and\n",
      "Great Northern lines hit . Planned works in Holloway area of London\n",
      "were not completed in time . Customers advised to  start and finish\n",
      "journey at Finsbury Park instead . Thousands expected to make post-\n",
      "Christmas return journey on Saturday . East Coast Trains apologised\n",
      "for cancellation on a 'very busy travel day' National Rail said\n",
      "services would be 'significantly disrupted' tomorrow .<EOS>\n"
     ]
    }
   ],
   "source": [
    "# prints: [Example][<EOS>][<pad>][Example Summary][<EOS>]\n",
    "print(f'Single example:\\n\\n {detokenize(train_input)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "muslim-desktop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs are in different lengths and padding them with 0s will wast computational resource so good approach \n",
    "# would be group the strings to specific sizes and process, ww use buckets to create batched generators\n",
    "# buckets are defined based on boundaries and batch sizes, batch size[i] signifies the \n",
    "# batch size for the items with length < boundaries[i], so we use batch size 4 of length<512, 8 of length<256,16 of sentence lewnght <128 so on\n",
    "\n",
    "boundaries =[128,256,512,1024]\n",
    "batch_size=[16,8,4,2,1]\n",
    "\n",
    "# now create the stream\n",
    "train_batch_stream=trax.data.BucketByLength(boundaries,batch_size)(train_stream)\n",
    "eval_batch_stream=trax.data.BucketByLength(boundaries,batch_size)(eval_stream)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "funky-craps",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1095)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# different articles will be produced every time\n",
    "input_batch,_,mask_batch=next(train_batch_stream)\n",
    "input_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "dangerous-language",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    9,  3890,  2726,  1442,  1475,    31, 13062,  1379,  1329,\n",
       "          132, 10448,  5403,   251,  4018,   691, 14850,    28, 14496,\n",
       "          340,  1733,     6,   112,  2365,    95,  4240, 14662,    17,\n",
       "         9012,  5121,  2796,  2301,  9528,    79,     3,  1803,  4219,\n",
       "            6,   605,  4970, 10492,  8338,  1619,   213,   344,     2,\n",
       "        13022,    16,   441,   694,   186, 15432,   595,  3890,  2726,\n",
       "         1442,    80,   269, 13628,  4961,   412,   444,     6,   179,\n",
       "         4970,  7372, 15301,   246,  3060,     2,   192,  2262,  7158,\n",
       "         5745,  5047,    58,     2, 12746,    75,     6,   605, 12929,\n",
       "            5, 16647,   186,   203,  1223,  4970,  7358,    43,  2696,\n",
       "         6948,     3,  2301,  9528,    79,  2262, 11728, 14920,    79,\n",
       "        20227, 26377,   471,  2696,    28,   532, 25222,   232,  1124,\n",
       "            2,  5684,   691,   492, 22100,     4,   186,  8349,  1831,\n",
       "         4219,     6,   605,  2814, 18685, 21351,     2,    35,    77,\n",
       "         1353,   445,  1019,   213, 13796,     5,   320,  8928,     4,\n",
       "           78,   297, 13614,    84,   505,  5479,  1019,   105,     3,\n",
       "         4970,  7372,  4390,    15,   270,     2,   186,  3890, 10230,\n",
       "            7,     5,  3646,     2,  1124,   527,   213,  1207,   132,\n",
       "         2754,  1353,    28, 10293,   802,    95,  2301,  9528,    79,\n",
       "          379,    52,  2482,   213, 11892, 25226,     4,  7874, 16264,\n",
       "            5,  4794,   225,  1386,   436,   263,  1851,   527, 12995,\n",
       "            4,  4876,     2, 26931,    16,    61, 19209,   114,   382,\n",
       "         1895,     7,     5, 20672,   214, 13943,    16,  9213,  2640,\n",
       "        24355,   859, 11115, 26107, 19940,   809,  6538,     7,     5,\n",
       "        12561,     3, 14859,    39,   787,   412, 12891,  9001,    70,\n",
       "           41,  2530,  3890,  2726,  1442,  3060,   132,   220,   357,\n",
       "            7,     5, 11583, 22096,    47,  1379,    70,    35, 10492,\n",
       "         8338,   186,   458,    49,   570,   320,   213,   644, 23912,\n",
       "            5,  9676,   527,   454,   163,  8404,     3,     9,    86,\n",
       "        19356,  1049,  1087,    78,   163,  2303, 10293,  4979,  1353,\n",
       "         1803, 16784,  6408, 14925,   991,  7358,   144,  1526,   236,\n",
       "        16752,  1822,   123,   213,   270,   422,   102,  5761,  2754,\n",
       "         1472,   320,  1151,    28,  8488,  2918,     3,   198,  1353,\n",
       "           28,   137,     6,  2425,  3712,  9178,   657,   192,  7358,\n",
       "          625,  1236,  1838, 11949,  1984,     3,  4970, 10492,  8338,\n",
       "         1619,   213,   344,     2, 13022,    16,   441,   694,   186,\n",
       "        15432,   595,  3890,  2726,  1442,    80,   269, 13628,  4961,\n",
       "          379,  2301,  9528,    79,     2,   179,   132, 18538,   445,\n",
       "           44,    74,    28,   984,   102,    41,  4133,    28,  1911,\n",
       "            6,   388,  4794,   225,  4376,   214,   213,  3890,  2726,\n",
       "         1442,     2,    25,  1132,   155,  1778,  7511, 10492,  8338,\n",
       "         8018,    17,    28,  6627,  5689,     6,  1287,  1838,  1577,\n",
       "         1599,   278,  2957,     3,   244,   849,  2301,  9528,    79,\n",
       "        27107,    17,   285,  1013,     2,    72,  1123, 10492,  8338,\n",
       "        17382,   145,   213,  1567,  1223,  1170,  9289,    17,   809,\n",
       "           28,   209,  3761,   320,   413,  1019,   213,  6844,     6,\n",
       "         4250,   248,   132,   824,   357,     7,     5,  1557,     3,\n",
       "          198,  1353,    28,  3663, 15144,  2685,  3890,  2726,  1442,\n",
       "           80,   385,   186,   102, 10492,  8338,  1393,    15,  7218,\n",
       "         7295,     6, 19467,  1599,   388,  1170,     2,    31,  1567,\n",
       "         1124,  2362,  4901, 10038,   355,   749,   284,     6,  1957,\n",
       "          385,     2,  4605,  1025,  8242,   186,    28, 16110,     4,\n",
       "        10492,  8338,  1885,    74,  1233,  5745,  5047,    58,    95,\n",
       "            3, 10492,  8338,   969,   213,  7125,     2,   186,  3890,\n",
       "         2726,  1442,    25,   110,    78,    31,   138,   320,    28,\n",
       "          775, 14496,     6,   340,  2365,     2,   971,   381,     6,\n",
       "          121,   263,   132,   213,   270,  2803,   527,    28,    36,\n",
       "            6, 18712,  8136,     3,  4970, 27439,  9275,  1628,  7358,\n",
       "         1353,  1526,   236, 16752,  1822,   123,   213,   270,   422,\n",
       "          102,  5761,  2754,  1472,   320,  1151,    28,  8488,  2918,\n",
       "          379,  2301,  9528,    79,     7,     5,  1275,  1353,    19,\n",
       "         1504,   691,   213,  1201,   123,  2918,   527,  4414,  8343,\n",
       "        12535,  4732,  9862,     2,   186,    41,    25,   146, 15912,\n",
       "         2012,   320,   373,   807,  7511, 12535,  4732,  9862,     7,\n",
       "            5,  6310, 16590,     4, 17019,    81,  1353,  6481,     6,\n",
       "         6701,  1560,   691,   624, 17676, 20942,    67,  6908,  9731,\n",
       "           14,  1019, 19277,   246,    28,  3540,  2412,  1577,  1599,\n",
       "           15,   248,     7,     5,   546,     3,  3890,  2726,  1442,\n",
       "          146,  1893,    61,   213,  1778,     2,   186, 16647, 26526,\n",
       "           17,    15,   138,    95,  1019,    28,  6020,  1124,   132,\n",
       "         1147,  3657,   824,   632,    70,   372,   285,  2377,    22,\n",
       "           39,  3439,  1901,  2958, 11206,   320,  4351,   213,  1803,\n",
       "          203,  1752, 10034,   214,  1567, 16784,  4394,   451,  7683,\n",
       "          703,    78,   594,   118,    70,   186, 10492,  8338,     7,\n",
       "            5,  7125,   133,   103,   612,     6,   121,     3,     9,\n",
       "         3890,  2726,  1442, 14729,     5,  2408,   320,  2253,    31,\n",
       "        14049,     4,   132,   213,   270,   422,     2,   186,    31,\n",
       "        16664,   375,   320,    28,   501,  2301,  9528,    79, 11401,\n",
       "         7905,   361,   285,   980, 13201,  6987,     4, 15147,  3764,\n",
       "         3256,   367,  4311,     6, 17086,    17,     2,  1480,   412,\n",
       "         1248, 17019,    81,     7,     5, 21280,   919,   213,  2796,\n",
       "        14434,     3, 13732, 11253,  6179,   153,   527,  3890,  2726,\n",
       "         1442,   229, 26271,    17,   691,  2301,  9528,    79,     7,\n",
       "            5,  1990,   321,   118,  3971,  9139,  7038,    79, 19147,\n",
       "         1853,   379,  4513,    28,  2425,   527,  3256,   367,     7,\n",
       "            5,  8931,     2,   213,  3890,  2726,  1442,  5485,  1019,\n",
       "           31,   604,  1124,  7511,  7372, 23200,  6259, 26881,    28,\n",
       "        24458,   527,  2488,     2,   186, 10492,  8338,     7,     5,\n",
       "         7125,   346,  2301,  9528,    79,   286,   694, 22374,  3505,\n",
       "           26,  1248,    44,    74,   286,  1170,  2267,     3,     9,\n",
       "        14496,     6,   340,  1124,  2362,  1248,   141,  2436,  1170,\n",
       "         2274,   412,  2301,  9528,    79,     7,     5,  6374,   233,\n",
       "          838, 10458,   320,    38,  1085,     2, 11591,  7358,   320,\n",
       "         2081,   163,  8581, 16035,  1407,   655,     6,   132,   171,\n",
       "        10492,  8338,  3101,    15,   571,   318,  1641,   935,   743,\n",
       "          691,  7525,    28,  8683,  1123,  5474,  1838,  1147,  3658,\n",
       "            3,  7358,     7,     5,  2918,   206,    19, 20791,    20,\n",
       "        17201,     4,   213,  3890,  2726,  1442,    80, 13789,     2,\n",
       "          186,    31,  3646,  1124,  2362,   441,  1170,  1838,    55,\n",
       "          355,    28,  7549,  2033,   186,    28,  3324,  3095,     6,\n",
       "         4944,    21,  4032,  1885,   691,   213,  4861, 16647,   320,\n",
       "          163, 12252, 12357,    21,  7372,     3,  2301,  9528,    79,\n",
       "         1606,  5633,   527,  2770,   186,  3661,     2,  1786,   213,\n",
       "        19058,   314,     2,    35,    31,  2034,     2, 20532,   186,\n",
       "         5875,  7537,  1134,   103,    39,  1151,    28,   407,  5258,\n",
       "          122,    41,   802,    28,   505,   282,   824,   357,     2,\n",
       "          683,   412,    31,    54,  3875,  7683,  1435, 26107, 19940,\n",
       "          186, 15955, 11773,     3,   512,   213,   282,   209,   757,\n",
       "          412,    28,  6187,     2,  3890,  2726,  1442,   570,  1476,\n",
       "         3228,  4893,  1581,    40,    38,  1223,   527,    15, 23425,\n",
       "            5,    78,   110,   171,   213,   250,     2,   176,  1803,\n",
       "        10688, 18987,     5, 13732, 11253,  6179,   153,     2, 12048,\n",
       "         6323,  8032,   186, 14142,  1673,     2,    35,    22,  3621,\n",
       "          213,   505,  1329,   229,  2685,   320,  9806,   173,    61,\n",
       "         9136,   412, 26107, 19940,  3789,   132,  3945,    10,     1,\n",
       "            0,  3890,  2726,  1442,  2530,  2301,  9528,    79,  1733,\n",
       "            6,   112,   132,    31, 27439,  9275,  1628, 13062, 27439,\n",
       "         9275,  1628,  1379, 25677,     4, 16346, 27439,  6774,  1628,\n",
       "         4970, 10492,  8338, 16456,   441,   694,   186,   510,   110,\n",
       "          799, 16346, 27439,  6774,  1628,  4970,  7372,     2,  7158,\n",
       "         5745,  5047,    58,     2, 12929,     5, 16647,   186,  4970,\n",
       "         7358,    38,  2696,  6948,  2104,     1]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see the corresponding iteger values\n",
    "input_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "double-iraqi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article and summary:\n",
      "\n",
      " The Ospreys opened their Champions Cup campaign in emphatic fashion by\n",
      "posting a bonus point 42-7 victory over outclassed Liberty Stadium\n",
      "visitors Treviso. Wales fly-half Dan Biggar ran the show, kicking 17\n",
      "points and orchestrating Ospreys' best attacking moments as full-back\n",
      "Dan Evans touched down twice, while wing Jeff Hassler, scrum-half Rhys\n",
      "Webb and number eight Dan Baker also scored tries. Treviso wing\n",
      "Ludovico Nitoglia scored a late consolation try, converted by former\n",
      "Worcester and Wasps fly-half Joe Carlisle, but there was little for\n",
      "the Italians to cheer on another dismal European occasion for them.\n",
      "Dan Evans scores his second, and Osprey's fifth, try of the match in\n",
      "what was a routine win over Treviso . It meant the unbeaten Guinness\n",
      "PRO12 leaders took early charge of Pool Five, teeing up nicely next\n",
      "Saturday's clash against reigning Aviva Premiership champions\n",
      "Northampton at Franklin's Gardens. Saints will start as favourites -\n",
      "they beat Ospreys twice in last season's Heineken Cup - but Biggar and\n",
      "company can head to the East Midlands confident of making an\n",
      "impression. The only worrying note on an otherwise routine afternoon\n",
      "was Wales autumn squad hopeful Baker being carried off midway through\n",
      "the second period after suffering what appeared to be a neck injury.\n",
      "There was a 10-minute stoppage while Baker received attention from\n",
      "medics. Dan Biggar ran the show, kicking 17 points and orchestrating\n",
      "Ospreys' best attacking moments . Treviso, back in Swansea little more\n",
      "than a month after they suffered a 44-13 PRO12 defeat against the\n",
      "Ospreys, were soon under pressure when Biggar sparked a brilliant\n",
      "counter-attack from deep inside home territory. And although Treviso\n",
      "halted that move, two successful Biggar penalties during the opening\n",
      "eight minutes hinted at a long shift to come for the lowest-ranked\n",
      "team in this season's competition. There was a fluency about Ospreys'\n",
      "play and after Biggar completed his penalty hat-trick inside 13\n",
      "minutes, their opening try arrived shortly afterwards following strong\n",
      "set-piece play, slick handling and a neat Biggar pass than sent\n",
      "Hassler over. Biggar added the conversion, and Ospreys were well on\n",
      "their way to a potential bonus-point victory, leading 16-0 early in\n",
      "the second quarter of a one-sided encounter. Dan Baker was carried off\n",
      "midway through the second period after suffering what appeared to be a\n",
      "neck injury . Treviso's cause was not helped by the loss through\n",
      "injury of captain Antonio Pavanello, and they were then temporarily\n",
      "reduced to 14 players when Pavanello's replacement Marco Fuser was\n",
      "sin-binned by French referee Pascal Gauzere for pulling down a maul\n",
      "deep inside his team's 22. Ospreys then kept up the pressure, and Webb\n",
      "sniped his way over for a sixth try in seven starts this term - form\n",
      "that suggests he will strongly challenge Mike Phillips to wear the\n",
      "Wales number nine shirt against opening autumn Test series opponents\n",
      "Australia on November 8 - and Biggar's conversion made it 23-0. The\n",
      "Ospreys forwards looked to maintain their grip in the second period,\n",
      "and their dominance led to a further Treviso indiscretion that saw\n",
      "substitute prop Salesi Manu yellow-carded, which as with Fuser's\n",
      "caution cost the visitors dear. Justin Tipuric of Ospreys is tackled\n",
      "by Treviso's Italian No 8 Alessandro Zanni . Within a minute of Manu's\n",
      "exit, the Ospreys struck for their third try when Evans grounded\n",
      "possession underneath a pile of bodies, and Biggar's conversion left\n",
      "Treviso 30 points adrift with more than 30 minutes remaining. The\n",
      "bonus-point try arrived with just 54 minutes gone as Treviso's defence\n",
      "found itself scattered to all parts, enabling Baker to enjoy an\n",
      "unopposed run-in before Biggar maintained his 100 per cent success\n",
      "rate by landing a seventh successful kick from seven attempts. Baker's\n",
      "injury did not noticeably disrupt the Ospreys' rhythm, and their fifth\n",
      "try arrived 17 minutes from time following a sharp break and a\n",
      "superbly-timed scoring pass by the impressive Webb to an unmarked\n",
      "Evans. Treviso showed plenty of commitment and desire, despite the\n",
      "scoreline, but their technical, tactical and skill limitations mean it\n",
      "will be a major surprise if they win a European game this season,\n",
      "especially as their other pool opponents are Northampton and Racing\n",
      "Metro. With the game long done as a contest, Ospreys head coach Steve\n",
      "Tandy had all eight of his substitutes on well before the end,\n",
      "including Wales internationals Justin Tipuric, Aaron Jarvis and Duncan\n",
      "Jones, but he knows the European campaign is about to crank up\n",
      "considerably as Northampton lie in wait.<EOS><pad>Ospreysbeat Treviso\n",
      "42-7 in their Champions Cup opener . Dan Biggar kicked 17 points and\n",
      "played well throughout . Dan Evans, Jeff Hassler, Rhys Webb and Dan\n",
      "Baker all scored tries .<EOS>\n"
     ]
    }
   ],
   "source": [
    "# above input batch consists:\n",
    "#     all the values corresponding to words, first 1 represents the <EOS> of the article followed by 0 (pads)\n",
    "#     after the first 0, other values show the summary words and the second 1 represent the <EOS> tag for summary\n",
    "#       All the other 0s are to maintain the consistancy of the  length - max length specified in the bucket \n",
    "# show the processes input data batch\n",
    "print(\"Article and summary:\\n\\n\",detokenize(input_batch[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "solid-consultation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so the structure is article <EOS><PADs>article summary <EOS><pads>\n",
    "# loss is taken only on the summary using cross entropy as loss function\n",
    "#  Now create helper functions to create tensor and to display tensors using jax numpy array\n",
    "\n",
    "def create_tensor(tensor):\n",
    "    \"\"\"input list of lists out put a tensor\"\"\"\n",
    "    return jnp.array(tensor)\n",
    "\n",
    "def display_tensor(tensor,name):\n",
    "    \"\"\" display the name and tensor\"\"\"\n",
    "    print(f'{name} shape: {tensor.shape}\\n')\n",
    "    print(f'{tensor}\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-digest",
   "metadata": {},
   "source": [
    "# try with dummy data and build attention - dot product\n",
    "$$\n",
    "\\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}+{M}\\right) V\\tag{1}\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "proved-complex",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dotproduct attention\n",
    "def DotProductAttention(query,key,value,mask):\n",
    "    \"\"\" dot product self attention\n",
    "    args: query - jax.intepreters.xla.DeviceArray: array of query representations with shape L_q by d\n",
    "    key jax.intepreters.xla.DeviceArray: array of query representation with shape L_k by d\n",
    "    value jax.intepreters.xla.DeviceArray: array of value representation with shpeL_k by d where L_v=L_k\n",
    "    mask jax.interpreters.sla.DeviceArray: attention mask, gates attention with shape L_q by L_k ( this is due to dot product)\n",
    "    returns jax.intperters.xla.DeviceArray: self-attention array for q,k,v arrays L_q by L_k\"\"\"\n",
    "    \n",
    "    assert query.shape[-1]==key.shape[-1]==value.shape[-1],\"Embeedding dimesions of q,k,v must be same\"\n",
    "#     get the depth dimentionality of the query embedding  for the scaling down the dot product\n",
    "    depth=query.shape[-1]\n",
    "    \n",
    "#     get the scaled query key dot product according to formula above\n",
    "    dots=jnp.amtmul(query,jnp.swapaxes(key,-1,-2))/jnp.sqrt(depth)\n",
    "    \n",
    "#     now apply the mask\n",
    "    if mask is None:\n",
    "        dots=jnp.where(mask,dots,jnp.full_like(dots,-1e9))\n",
    "    \n",
    "#     softmax\n",
    "    logsumexp=trax.fastmath.logsumexp(dots,axis=-1, keepdims=True)\n",
    "    \n",
    "#     now get the exponnential of dots minus logsumexp to get softmax\n",
    "    dots=jnp.exp(dots-logsumexp)\n",
    "#      now multiply by values to get the attention\n",
    "    attention=jnp.matmul(dots,vlaue)\n",
    "    \n",
    "    return attention\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "expired-coaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now inpmelent the causal attention: multi headed attention with mask to attend only the words that occured before\n",
    "# 1. copute attention heads - get input with dimention (batch size, seqlen,n_heads X d_head) then splits the last(depth)\n",
    "# dimension and stacks it to the zeroth dimension to allow matrix multiplication (batch_size X n_heads,seqlen,d_head)\n",
    "\n",
    "# 2. dot product self attention\n",
    "# 3. compute attention output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "chronic-singles",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_attention_heads_closure(n_heads,d_head):\n",
    "    \"\"\"Function that simulates environment inside CasualAttention \n",
    "    function.\n",
    "    Args: \n",
    "    d_head(int): dimensionality of heads.\n",
    "    n_heads (int): number of attention heads\n",
    "    Returns:\n",
    "    function: compute_attention_heads function\"\"\"\n",
    "    \n",
    "    def compute_attention_heads(x):\n",
    "        \"\"\"Compute attention heads.\n",
    "        Args:\n",
    "            x (jax.interpreters.xla.DeviceArray): tensor with shape(batch_size,\n",
    "            seqlen,n_heads X d_head)\n",
    "        Returns:\n",
    "            jax.interpreters.xla.DeviceArray: reshaped tensor with shape (batch_size X n_heads, seqlen,d_head).\n",
    "        \"\"\"\n",
    "        #x batch dimension\n",
    "        batch_size=x.shape[0]\n",
    "        #length of sequence should be size of x's first dimension without counting batch dim\n",
    "        seqlen=x.shape[1]\n",
    "        #now change the shape from batch_size,seqlen,n_heads*d_head to batch_size,seqlen,n_heads,d_head\n",
    "        x=jnp.reshape(x,(batch_size,seqlen,n_heads,d_head))\n",
    "        #then transpose batch_size,seqlen,n_heads,d_head-->batch_size,n_heads,seqlen,d_head\n",
    "        #here the values within the tuple  are the indexes of the dimensions of x and need to rearrange them\n",
    "        x=jnp.transpose(x,(0,2,1,3))\n",
    "        #now reshape to batch_size,n_heads,seqlen,d_head -->batch_size*n_heads,seqlen,d_head\n",
    "        x=jnp.reshape(x,(-1,seqlen,d_head))\n",
    "        return x\n",
    "    return compute_attention_heads\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "generous-standard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create the dotproduct self attention with mask\n",
    "def dot_product_self_attention(q,k,v):\n",
    "    \"\"\"Masked dot product self attention\n",
    "    args: q (jax.interpreters.xla.DeviceArray): queries.\n",
    "          k (jax.interpreters.xla.DeviceArray):keys.\n",
    "          v (jax.interpreters.xla.DeviceArray):values\n",
    "    Returns:\n",
    "        jax.interpreters.xla.DeviceArray: masked dot product self attention tensor\"\"\"\n",
    "#     mask size should be size of L_q. q has shape of (batch_size,L_q,d)\n",
    "    mask_size=q.shape[-2]\n",
    "    # Creates a matrix with ones below the diagonal and 0s above. It should have shape (1, mask_size, mask_size)\n",
    "    # Notice that 1's and 0's get casted to True/False by setting dtype to jnp.bool_\n",
    "    # Use jnp.tril() - Lower triangle of an array and jnp.ones()\n",
    "    mask = jnp.tril(jnp.ones((1, mask_size, mask_size), dtype=jnp.bool_), k=0)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return DotProductAttention(q, k, v, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-hopkins",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
